{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13953756,"sourceType":"datasetVersion","datasetId":8894072}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torchvision.models import MobileNet_V3_Small_Weights\nimport numpy as np\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom skimage import io, exposure\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport torchvision.transforms as transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:09.571428Z","iopub.execute_input":"2025-12-07T07:50:09.571912Z","iopub.status.idle":"2025-12-07T07:50:24.002315Z","shell.execute_reply.started":"2025-12-07T07:50:09.571887Z","shell.execute_reply":"2025-12-07T07:50:24.001682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SEBlock(nn.Module):\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, channels // reduction, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return x * self.fc(x)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n        padding = (kernel_size - 1) // 2\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        # Compute channel-wise max and mean\n        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n        mean_pool = torch.mean(x, dim=1, keepdim=True)\n        attn = torch.cat([max_pool, mean_pool], dim=1)\n        attn = self.sigmoid(self.conv(attn))\n        return x * attn\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_ch, skip_ch, out_ch, use_se=True, use_sa=True):\n        super().__init__()\n       \n        self.conv1 = nn.Conv2d(in_ch + skip_ch, out_ch, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_ch)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SEBlock(out_ch) if use_se else nn.Identity()\n        self.sa = SpatialAttention() if use_sa else nn.Identity()\n    def forward(self, x, skip):\n        x = F.interpolate(x, size=skip.shape[2:], mode=\"bilinear\", align_corners=False)\n        x = torch.cat([x, skip], dim=1)\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        x = self.se(x) # channel attention\n        x = self.sa(x) # spatial attention\n        return x\n\nclass LMA_UNet(nn.Module):\n    def __init__(self, n_classes=2, in_channels=4):  # Changed to n_classes=2 for binary\n        super().__init__()\n        self.n_classes = n_classes\n        mobilenet = models.mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n        if in_channels !=3:\n            mobilenet.features[0][0] = nn.Conv2d(in_channels,16,3,2,1,bias=False)\n        self.encoder = mobilenet.features\n        self.skip_idx = [0,1,2,8]\n        self.dec4 = DecoderBlock(576, 48, 256, use_se=True, use_sa=True)\n        self.dec3 = DecoderBlock(256, 24, 128, use_se=True, use_sa=True)\n        self.dec2 = DecoderBlock(128, 16, 64, use_se=True, use_sa=True)\n        self.dec1 = DecoderBlock(64, 16, 32, use_se=True, use_sa=True)\n        self.head = nn.Conv2d(32,n_classes,1)\n    def forward(self,x):\n        skips = []\n        for i,l in enumerate(self.encoder):\n            x = l(x)\n            if i in self.skip_idx: skips.append(x)\n        bottleneck = x\n        skips = skips[::-1]\n        x = self.dec4(bottleneck, skips[0])\n        x = self.dec3(x, skips[1])\n        x = self.dec2(x, skips[2])\n        x = self.dec1(x, skips[3])\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        x = self.head(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:26.271957Z","iopub.execute_input":"2025-12-07T07:50:26.272423Z","iopub.status.idle":"2025-12-07T07:50:26.285867Z","shell.execute_reply.started":"2025-12-07T07:50:26.272397Z","shell.execute_reply":"2025-12-07T07:50:26.285195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:28.941430Z","iopub.execute_input":"2025-12-07T07:50:28.941718Z","iopub.status.idle":"2025-12-07T07:50:29.030216Z","shell.execute_reply.started":"2025-12-07T07:50:28.941697Z","shell.execute_reply":"2025-12-07T07:50:29.029541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Binary Dice Coefficient\ndef dice_coefficient(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1]  # Probability for tumor class (1)\n    pred = (pred > 0.5).float()  # Threshold to binary\n    target = (target == 1).float()  # Binary target: 1 for tumor, 0 for background\n    \n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum()\n    dice = (2 * intersection + smooth) / (union + smooth)\n    return dice\n\n# Binary IoU\ndef iou_score(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1]  # Probability for tumor class\n    pred = (pred > 0.5).float()\n    target = (target == 1).float()\n    \n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum() - intersection\n    iou = (intersection + smooth) / (union + smooth)\n    return iou\n\n# Binary Sensitivity (Recall)\ndef sensitivity_score(pred, target, smooth=1e-6):\n    pred = torch.softmax(pred, dim=1)[:, 1]\n    pred = (pred > 0.5).float()\n    target = (target == 1).float()\n    \n    tp = (pred * target).sum()\n    fn = target.sum() - tp\n    sens = tp / (tp + fn + smooth)\n    return sens\n\n# Binary loss: Dice + BCE\nclass DiceBCELoss(nn.Module):\n    def __init__(self, dice_weight=1.0, bce_weight=1.0, smooth=1e-6):\n        super().__init__()\n        self.dice_weight = dice_weight\n        self.bce_weight = bce_weight\n        self.smooth = smooth\n        self.bce = nn.CrossEntropyLoss()\n\n    def forward(self, pred, target):\n        bce_loss = self.bce(pred, target)\n        \n        pred_soft = torch.softmax(pred, dim=1)[:, 1]\n        target_bin = (target == 1).float()\n        \n        intersection = (pred_soft * target_bin).sum()\n        union = pred_soft.sum() + target_bin.sum()\n        dice_loss = 1 - (2 * intersection + self.smooth) / (union + self.smooth)\n        \n        return self.dice_weight * dice_loss + self.bce_weight * bce_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:29.906230Z","iopub.execute_input":"2025-12-07T07:50:29.906981Z","iopub.status.idle":"2025-12-07T07:50:29.915863Z","shell.execute_reply.started":"2025-12-07T07:50:29.906955Z","shell.execute_reply":"2025-12-07T07:50:29.915107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BRISCDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = sorted(os.listdir(image_dir))\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.images[idx])\n        # Extract base name from image (.jpg) and append .png for mask\n        base_name = os.path.splitext(self.images[idx])[0]\n        mask_filename = base_name + '.png'\n        mask_path = os.path.join(self.mask_dir, mask_filename)\n        \n        # Safety check\n        if not os.path.exists(mask_path):\n            raise FileNotFoundError(f\"Mask not found: {mask_path} (for image: {img_path})\")\n        \n        image = io.imread(img_path)  # Shape: (H, W) for grayscale MRI\n        mask = io.imread(mask_path, as_gray=True).astype(np.uint8)  # Shape: (H, W), original labels 0-3\n        \n        # Remap to binary: 0 -> 0 (background), 1/2/3 -> 1 (tumor)\n        mask = np.where(mask > 0, 1, 0).astype(np.uint8)\n        \n        # CHANGE: Replicate grayscale to 3 channels for CLAHE compatibility (was 4)\n        if len(image.shape) == 2:\n            image = np.stack([image] * 3, axis=-1)\n        \n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed['image']\n            mask = transformed['mask'].long()  # Convert to long after ToTensorV2\n        else:\n            # Manual conversion if no transform\n            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n            mask = torch.from_numpy(mask).long()\n        \n        return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:31.604422Z","iopub.execute_input":"2025-12-07T07:50:31.604705Z","iopub.status.idle":"2025-12-07T07:50:31.612348Z","shell.execute_reply.started":"2025-12-07T07:50:31.604684Z","shell.execute_reply":"2025-12-07T07:50:31.611562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 256  # Or 512, whatever you want\n\ntrain_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n    ToTensorV2()\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:32.868842Z","iopub.execute_input":"2025-12-07T07:50:32.869412Z","iopub.status.idle":"2025-12-07T07:50:32.881914Z","shell.execute_reply.started":"2025-12-07T07:50:32.869382Z","shell.execute_reply":"2025-12-07T07:50:32.881060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_image_dir = '/kaggle/input/brisc-cleaned/train/images'\ntrain_mask_dir = '/kaggle/input/brisc-cleaned/train/masks'\nval_image_dir = '/kaggle/input/brisc-cleaned/test/images'\nval_mask_dir = '/kaggle/input/brisc-cleaned/test/masks'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:33.043111Z","iopub.execute_input":"2025-12-07T07:50:33.043379Z","iopub.status.idle":"2025-12-07T07:50:33.047195Z","shell.execute_reply.started":"2025-12-07T07:50:33.043358Z","shell.execute_reply":"2025-12-07T07:50:33.046560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = BRISCDataset(train_image_dir, train_mask_dir, transform=train_transform)\nval_dataset = BRISCDataset(val_image_dir, val_mask_dir, transform=val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:34.550030Z","iopub.execute_input":"2025-12-07T07:50:34.550358Z","iopub.status.idle":"2025-12-07T07:50:34.849866Z","shell.execute_reply.started":"2025-12-07T07:50:34.550336Z","shell.execute_reply":"2025-12-07T07:50:34.849043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train dataset: {len(train_dataset)}\")\nprint(f\"Test dataset: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:35.684373Z","iopub.execute_input":"2025-12-07T07:50:35.684667Z","iopub.status.idle":"2025-12-07T07:50:35.689037Z","shell.execute_reply.started":"2025-12-07T07:50:35.684646Z","shell.execute_reply":"2025-12-07T07:50:35.688472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = LMA_UNet(n_classes=2, in_channels=3).to(device)\ncriterion = DiceBCELoss(dice_weight=1.5, bce_weight=1.0)  # Heavier on Dice for imbalance\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:36.912181Z","iopub.execute_input":"2025-12-07T07:50:36.912928Z","iopub.status.idle":"2025-12-07T07:50:37.448923Z","shell.execute_reply.started":"2025-12-07T07:50:36.912903Z","shell.execute_reply":"2025-12-07T07:50:37.447995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 100\nbest_dice = 0.0\npatience = 15\nearly_stop_counter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:50:39.312358Z","iopub.execute_input":"2025-12-07T07:50:39.312624Z","iopub.status.idle":"2025-12-07T07:50:39.316379Z","shell.execute_reply.started":"2025-12-07T07:50:39.312608Z","shell.execute_reply":"2025-12-07T07:50:39.315780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\ntrain_dices = []\nval_losses = []\nval_dices = []\nval_ious = []\nval_sens_list = []\n\nbest_dice = 0.0\nearly_stop_counter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:56:06.703723Z","iopub.execute_input":"2025-12-07T07:56:06.704499Z","iopub.status.idle":"2025-12-07T07:56:06.708745Z","shell.execute_reply.started":"2025-12-07T07:56:06.704469Z","shell.execute_reply":"2025-12-07T07:56:06.708077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for epoch in range(num_epochs):\n#     # Train\n#     model.train()\n#     train_loss = 0.0\n#     train_dice = 0.0\n#     pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n#     for images, masks in pbar:\n#         images, masks = images.to(device), masks.to(device)\n        \n#         optimizer.zero_grad()\n#         outputs = model(images)\n#         loss = criterion(outputs, masks)\n#         loss.backward()\n#         optimizer.step()\n        \n#         train_loss += loss.item()\n#         train_dice += dice_coefficient(outputs, masks).item()\n        \n#         pbar.set_postfix({'Loss': f'{loss.item():.4f}', 'Dice': f'{dice_coefficient(outputs, masks).item():.4f}'})\n    \n#     train_loss /= len(train_loader)\n#     train_dice /= len(train_loader)\n    \n#     # Validate\n#     model.eval()\n#     val_loss = 0.0\n#     val_dice = 0.0\n#     val_iou = 0.0\n#     val_sens = 0.0\n#     with torch.no_grad():\n#         for images, masks in val_loader:\n#             images, masks = images.to(device), masks.to(device)\n#             outputs = model(images)\n#             loss = criterion(outputs, masks)\n            \n#             val_loss += loss.item()\n#             val_dice += dice_coefficient(outputs, masks).item()\n#             val_iou += iou_score(outputs, masks).item()\n#             val_sens += sensitivity_score(outputs, masks).item()\n    \n#     val_loss /= len(val_loader)\n#     val_dice /= len(val_loader)\n#     val_iou /= len(val_loader)\n#     val_sens /= len(val_loader)\n    \n#     scheduler.step(val_dice)\n    \n#     # Store metrics\n#     train_losses.append(train_loss)\n#     train_dices.append(train_dice)\n#     val_losses.append(val_loss)\n#     val_dices.append(val_dice)\n#     val_ious.append(val_iou)\n#     val_sens.append(val_sens)\n    \n#     print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}')\n#     print(f'Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val IoU: {val_iou:.4f}, Val Sens: {val_sens:.4f}')\n    \n#     # Save best model\n#     if val_dice > best_dice:\n#         best_dice = val_dice\n#         torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'best_dice': best_dice,\n#         }, 'best_lma_unet_binary.pth')\n#         early_stop_counter = 0\n#         print(f'New best Dice: {best_dice:.4f} - Model saved!')\n#     else:\n#         early_stop_counter += 1\n    \n#     # Early stopping\n#     if early_stop_counter >= patience:\n#         print('Early stopping triggered!')\n#         break\n\n# print(f'Training completed. Best validation Dice: {best_dice:.4f}')\n\nfor epoch in range(num_epochs):\n    # ----------------- TRAIN ----------------- #\n    model.train()\n    train_loss_epoch = 0.0\n    train_dice_epoch = 0.0\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n    \n    for images, masks in pbar:\n        images, masks = images.to(device), masks.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss_epoch += loss.item()\n        train_dice_epoch += dice_coefficient(outputs, masks).item()\n        \n        pbar.set_postfix({'Loss': f'{loss.item():.4f}', \n                          'Dice': f'{dice_coefficient(outputs, masks).item():.4f}'})\n    \n    train_loss_epoch /= len(train_loader)\n    train_dice_epoch /= len(train_loader)\n    \n    # ----------------- VALIDATE ----------------- #\n    model.eval()\n    val_loss_epoch = 0.0\n    val_dice_epoch = 0.0\n    val_iou_epoch = 0.0\n    val_sens_epoch = 0.0\n    \n    with torch.no_grad():\n        for images, masks in val_loader:\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            val_loss_epoch += loss.item()\n            val_dice_epoch += dice_coefficient(outputs, masks).item()\n            val_iou_epoch += iou_score(outputs, masks).item()\n            val_sens_epoch += sensitivity_score(outputs, masks).item()\n    \n    val_loss_epoch /= len(val_loader)\n    val_dice_epoch /= len(val_loader)\n    val_iou_epoch /= len(val_loader)\n    val_sens_epoch /= len(val_loader)\n    \n    scheduler.step(val_dice_epoch)\n    \n    # ----------------- STORE METRICS ----------------- #\n    train_losses.append(train_loss_epoch)\n    train_dices.append(train_dice_epoch)\n    val_losses.append(val_loss_epoch)\n    val_dices.append(val_dice_epoch)\n    val_ious.append(val_iou_epoch)\n    val_sens_list.append(val_sens_epoch)\n    \n    # ----------------- PRINT ----------------- #\n    print(f'Epoch {epoch+1}: Train Loss: {train_loss_epoch:.4f}, Train Dice: {train_dice_epoch:.4f}')\n    print(f'Val Loss: {val_loss_epoch:.4f}, Val Dice: {val_dice_epoch:.4f}, Val IoU: {val_iou_epoch:.4f}, Val Sens: {val_sens_epoch:.4f}')\n    \n    # ----------------- SAVE BEST MODEL ----------------- #\n    if val_dice_epoch > best_dice:\n        best_dice = val_dice_epoch\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_dice': best_dice,\n        }, 'best_lma_unet_binary.pth')\n        early_stop_counter = 0\n        print(f'New best Dice: {best_dice:.4f} - Model saved!')\n    else:\n        early_stop_counter += 1\n    \n    # ----------------- EARLY STOPPING ----------------- #\n    if early_stop_counter >= patience:\n        print('Early stopping triggered!')\n        break\n\nprint(f'Training completed. Best validation Dice: {best_dice:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T07:56:08.138299Z","iopub.execute_input":"2025-12-07T07:56:08.139009Z","iopub.status.idle":"2025-12-07T08:42:14.336068Z","shell.execute_reply.started":"2025-12-07T07:56:08.138983Z","shell.execute_reply":"2025-12-07T08:42:14.335065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = range(1, len(train_losses) + 1)\n\nfig, axs = plt.subplots(2, 3, figsize=(18, 12))\n\n# Train and Val Loss\naxs[0, 0].plot(epochs, train_losses, 'b-', label='Train Loss')\naxs[0, 0].plot(epochs, val_losses, 'r-', label='Val Loss')\naxs[0, 0].set_title('Loss Curves')\naxs[0, 0].set_xlabel('Epochs')\naxs[0, 0].set_ylabel('Loss')\naxs[0, 0].legend()\naxs[0, 0].grid(True)\n\n# Train and Val Dice\naxs[0, 1].plot(epochs, train_dices, 'b-', label='Train Dice')\naxs[0, 1].plot(epochs, val_dices, 'r-', label='Val Dice')\naxs[0, 1].set_title('Dice Coefficient Curves')\naxs[0, 1].set_xlabel('Epochs')\naxs[0, 1].set_ylabel('Dice')\naxs[0, 1].legend()\naxs[0, 1].grid(True)\n\n# Val IoU\naxs[0, 2].plot(epochs, val_ious, 'g-', label='Val IoU')\naxs[0, 2].set_title('IoU Curve')\naxs[0, 2].set_xlabel('Epochs')\naxs[0, 2].set_ylabel('IoU')\naxs[0, 2].legend()\naxs[0, 2].grid(True)\n\n# Val Sensitivity\naxs[1, 0].plot(epochs, val_sens_list, 'm-', label='Val Sensitivity')\naxs[1, 0].set_title('Sensitivity Curve')\naxs[1, 0].set_xlabel('Epochs')\naxs[1, 0].set_ylabel('Sensitivity')\naxs[1, 0].legend()\naxs[1, 0].grid(True)\n\n# Combined Metrics (Val Dice, IoU, Sens)\naxs[1, 1].plot(epochs, val_dices, 'r-', label='Val Dice')\naxs[1, 1].plot(epochs, val_ious, 'g-', label='Val IoU')\naxs[1, 1].plot(epochs, val_sens_list, 'm-', label='Val Sensitivity')\naxs[1, 1].set_title('Validation Metrics Comparison')\naxs[1, 1].set_xlabel('Epochs')\naxs[1, 1].set_ylabel('Score')\naxs[1, 1].legend()\naxs[1, 1].grid(True)\n\nplt.tight_layout()\nplt.savefig('training_metrics_binary.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T08:45:08.599939Z","iopub.execute_input":"2025-12-07T08:45:08.600266Z","iopub.status.idle":"2025-12-07T08:45:10.544634Z","shell.execute_reply.started":"2025-12-07T08:45:08.600230Z","shell.execute_reply":"2025-12-07T08:45:10.543798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nnum_samples = 4\nindices = random.sample(range(len(val_dataset)), num_samples)\n\nwith torch.no_grad():\n    for k, idx in enumerate(indices):\n        image, mask = val_dataset[idx]\n\n        image = image.unsqueeze(0).to(device)  # add batch dimension\n        mask = mask.to(device)\n\n        output = model(image)\n        pred = torch.argmax(output, dim=1)[0].cpu().numpy()\n\n        # denormalize single image\n        orig = denormalize_image(image[0])[:3].permute(1,2,0).numpy()\n\n        # plot\n        fig, axs = plt.subplots(1,3, figsize=(15,5))\n        axs[0].imshow(orig)\n        axs[0].set_title(f\"Original Image #{k+1}\")\n        axs[0].axis('off')\n\n        axs[1].imshow(mask.cpu().numpy(), cmap='gray')\n        axs[1].set_title(\"Ground Truth\")\n        axs[1].axis('off')\n\n        axs[2].imshow(pred, cmap='gray')\n        axs[2].set_title(\"Prediction\")\n        axs[2].axis('off')\n\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T08:48:43.893032Z","iopub.execute_input":"2025-12-07T08:48:43.893349Z","iopub.status.idle":"2025-12-07T08:48:45.269127Z","shell.execute_reply.started":"2025-12-07T08:48:43.893326Z","shell.execute_reply":"2025-12-07T08:48:45.268474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# DICE FUNCTION\n# -----------------------\ndef dice_score(pred, target, eps=1e-6):\n    # pred, target: numpy arrays of shape [H, W] with 0/1 values\n    pred = torch.tensor(pred).float()\n    target = torch.tensor(target).float()\n\n    intersection = (pred * target).sum()\n    union = pred.sum() + target.sum()\n\n    dice = (2 * intersection + eps) / (union + eps)\n    return dice.item()\n\n\n# -----------------------\n# LOAD MODEL\n# -----------------------\ncheckpoint = torch.load('best_lma_unet_binary.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Inverse normalization\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\nstd  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n\ndef denormalize_image(img_tensor):\n    img = img_tensor.detach().cpu().clone()\n    img[:3] = img[:3] * std + mean\n    return torch.clamp(img, 0, 1)\n\n\n# -----------------------\n# VISUALIZATION + DICE\n# -----------------------\nnum_samples = 4\nindices = random.sample(range(len(val_dataset)), num_samples)\n\nwith torch.no_grad():\n    for k, idx in enumerate(indices):\n        image, mask = val_dataset[idx]   # no batch dimension\n\n        image_batch = image.unsqueeze(0).to(device)\n        mask = mask.to(device)\n\n        output = model(image_batch)\n        pred = torch.argmax(output, dim=1)[0].cpu().numpy()\n\n        # Convert GT mask to numpy\n        gt = mask.cpu().numpy()\n\n        # Compute Dice\n        dice = dice_score(pred, gt)\n\n        # Denormalized original\n        orig = denormalize_image(image)[:3].permute(1,2,0).numpy()\n\n        # ---- PLOT ----\n        fig, axs = plt.subplots(1, 3, figsize=(15,5))\n\n        axs[0].imshow(orig)\n        axs[0].set_title(f\"Original Image #{k+1}\")\n        axs[0].axis(\"off\")\n\n        axs[1].imshow(gt, cmap=\"gray\")\n        axs[1].set_title(\"Ground Truth\")\n        axs[1].axis(\"off\")\n\n        axs[2].imshow(pred, cmap=\"gray\")\n        axs[2].set_title(f\"Prediction\\nDice: {dice:.4f}\")\n        axs[2].axis(\"off\")\n\n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T08:51:32.602261Z","iopub.execute_input":"2025-12-07T08:51:32.602905Z","iopub.status.idle":"2025-12-07T08:51:34.505989Z","shell.execute_reply.started":"2025-12-07T08:51:32.602874Z","shell.execute_reply":"2025-12-07T08:51:34.505100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    sample_images = next(iter(val_loader))[0].to(device)\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    _ = model(sample_images)\n    end.record()\n    torch.cuda.synchronize()\n    inference_time_ms = start.elapsed_time(end)\n    print(f'Average inference time per image (batch=8): {inference_time_ms / 8:.2f} ms')\n    print('Model is lightweight with MobileNetV3 backbone, ensuring fast inference (~10-20 FPS on GPU).')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T08:48:52.882756Z","iopub.execute_input":"2025-12-07T08:48:52.883548Z","iopub.status.idle":"2025-12-07T08:48:53.465382Z","shell.execute_reply.started":"2025-12-07T08:48:52.883521Z","shell.execute_reply":"2025-12-07T08:48:53.464414Z"}},"outputs":[],"execution_count":null}]}